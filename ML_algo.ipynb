{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wch.github.io/latexsheet/latexsheet.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>Introduction to machine learning</li>\n",
    "    <li> Supervised ML and Unsupervised ML</li>\n",
    "    <li>Linear Regression</li>\n",
    "    <li>$R^2$ and Abjusted $R^2$</li>\n",
    "    <li>Ridge and Lasso Regression</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Machine Learning <br>\n",
    "\n",
    "\n",
    "#### <u>AI application</u>\n",
    "    AI application is able to do its own tasks without any human intervention.\n",
    "    eg. Netflix, Amazon, self-driving cars\n",
    "    \n",
    "#### <u>Machine learning</u>\n",
    "    Machine learning is a subset of AI that used statistics to analyze, visualize, prediction and forecasting.\n",
    "   \n",
    "#### <u>Deep learning</u>\n",
    "    Deep learning is a subset to machine learning that mimics human's brain through multilayers neural network.\n",
    "    \n",
    "In machine learning and deep learning, there are supervised learning, unsupervised learning and reinforcement. In supervised learning, there are regression, and classification techniques. In unsupervised learning, there are clustering, and dimensionality reduction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Learning\n",
    "\n",
    "Models are built based on the independent features (inputs) and dependent features (outputs)\n",
    "\n",
    "##### <u>Regression</u> \n",
    "    If the dependent features (outputs) is continuous, then the problem becomes a regression problem\n",
    "    \n",
    "##### <u>Classification</u>\n",
    "    If the dependent features (outputs) is a fixed number of categories, then the problem becomes a classification problem.\n",
    "    eg. binary classification (2 categories)\n",
    "        multiclass classification (more than 2 categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Unsupervised Learning\n",
    "\n",
    "Models are built based on the independent features. Here there are no dependent features\n",
    "\n",
    "#####  <u>Clustering</u>\n",
    "    Clustering groups observations with similar features into groups\n",
    "    e.g. customer segmentation\n",
    "#####  <u>Dimensionality Reduction</u>\n",
    "    Dimensionality reduction reduces higher dimension to lower dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithms will be discussed <br>\n",
    "\n",
    "##### Supervised learning  \n",
    "<ol>\n",
    "    <li>Linear Regression</li>\n",
    "    <li>Ridge and Lasso</li>\n",
    "    <li>Logistic Regression</li>\n",
    "    <li>Decision Tree</li>\n",
    "    <li>AdaBoost</li>\n",
    "    <li>Random Forest</li>\n",
    "    <li>Gradient Boosting</li>\n",
    "    <li>Xgboost</li>\n",
    "    <li>Naive Bayes</li>\n",
    "    <li>Support Vector Machine, SVM</li>\n",
    "    <li>KNN</li>\n",
    "</ol>\n",
    "<br>\n",
    "\n",
    "##### Unsupervised learning  \n",
    "<ol>\n",
    "    <li>K means</li>\n",
    "    <li>DBScan</li>\n",
    "    <li>Hierarchical Clustering</li>\n",
    "    <li>K Nearest Clustering</li>\n",
    "    <li>Principal Component Analysis, PCA</li>\n",
    "    <li>Latent Dirichlet Allocation, LDA</li>\n",
    "    <li>Gradient Boosting</li>\n",
    "    <li>Xgboost</li>\n",
    "    <li>Naive Bayes</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Linear Regression\n",
    "<br>\n",
    "  \n",
    "  Train dataset --> Model --> Hypothesis (new input --> predict output)\n",
    "  \n",
    "  Model : $ h_{\\theta}(x) = \\theta_0 + \\theta_1x$ \n",
    " \n",
    " where $\\theta_0$ and  $\\theta_1$ are weights\n",
    "\n",
    "  Purpose is the minimize the distance between the data points and the best fit line.\n",
    "  \n",
    "  <u>Cost function</u>\n",
    "  \n",
    "  $J(\\theta_0,\\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})^2$\n",
    "  \n",
    "  This cost function is called Squared Error Function\n",
    "  \n",
    "$$minimize_{\\theta_0,\\theta_1}    \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})^2$$\n",
    " \n",
    "$$minimize_{\\theta_0,\\theta_1}    J(\\theta_0,\\theta_1)$$\n",
    "\n",
    "![cost function graph](cost_curve.png \"cost function graph\")\n",
    "\n",
    " <u>Convergence Algorithms</u>\n",
    " \n",
    " repeat until convergence\n",
    " \n",
    "   {\n",
    " \n",
    " $$\\theta_j := \\theta_j - \\alpha \\frac{\\delta}{\\delta\\alpha_j}J(\\theta_0,\\theta_1)$$\n",
    " \n",
    "   }\n",
    "   \n",
    "$\\alpha$ is the learning rate, usually 0.01.\n",
    "\n",
    "<u>Gradient Descrent Algorithm</u>\n",
    "\n",
    " repeat until convergence\n",
    " \n",
    "   {\n",
    " \n",
    " $$\\theta_j := \\theta_j - \\alpha \\frac{\\delta}{\\delta\\alpha_j}J(\\theta_0,\\theta_1)$$\n",
    " \n",
    "   }\n",
    "   \n",
    "   $\\frac{\\delta}{\\delta\\alpha_j}J(\\theta_0,\\theta_1) = \\frac{\\delta}{\\delta\\alpha_j} \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})^2$\n",
    "<br><br>\n",
    "   \n",
    "$ j = 0$ <br>\n",
    "   $$\\frac{\\delta}{\\delta\\alpha_0}J(\\theta_0,\\theta_1) = \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})$$\n",
    "   \n",
    "<br><br>\n",
    "$ j = 1 $<br>\n",
    "   $$\\frac{\\delta}{\\delta\\alpha_1}J(\\theta_0,\\theta_1) = \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})x^{(i)}$$\n",
    "<br><br>   \n",
    "repeat until convergence\n",
    " \n",
    "   {\n",
    " \n",
    " $$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})$$ <br>\n",
    " $$\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})x^{(i)}$$\n",
    " \n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
