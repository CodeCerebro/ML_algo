{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wch.github.io/latexsheet/latexsheet.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>Introduction to machine learning</li>\n",
    "    <li> Supervised ML and Unsupervised ML</li>\n",
    "    <li>Linear Regression</li>\n",
    "    <li>$R^2$ and Abjusted $R^2$</li>\n",
    "    <li>Ridge and Lasso Regression</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Machine Learning <br>\n",
    "\n",
    "\n",
    "#### <u>AI application</u>\n",
    "    AI application is able to do its own tasks without any human intervention.\n",
    "    eg. Netflix, Amazon, self-driving cars\n",
    "    \n",
    "#### <u>Machine learning</u>\n",
    "    Machine learning is a subset of AI that used statistics to analyze, visualize, predict and forecast.\n",
    "   \n",
    "#### <u>Deep learning</u>\n",
    "    Deep learning is a subset to machine learning that mimics human's brain through multilayers neural network.\n",
    "    \n",
    "In machine learning and deep learning, there are supervised learning, unsupervised learning and reinforcement. In supervised learning, there are regression, and classification techniques. In unsupervised learning, there are clustering, and dimensionality reduction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Learning\n",
    "\n",
    "Models are built based on the independent features (inputs) and dependent features (outputs)\n",
    "\n",
    "##### <u>Regression</u> \n",
    "    If the dependent features (outputs) is continuous, then the problem becomes a regression problem\n",
    "    \n",
    "##### <u>Classification</u>\n",
    "    If the dependent features (outputs) is a fixed number of categories, then the problem becomes a classification problem.\n",
    "    eg. binary classification (2 categories)\n",
    "        multiclass classification (more than 2 categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Unsupervised Learning\n",
    "\n",
    "Models are built based on the independent features. Here there are no dependent features\n",
    "\n",
    "#####  <u>Clustering</u>\n",
    "    Clustering groups observations with similar features into groups\n",
    "    e.g. customer segmentation\n",
    "#####  <u>Dimensionality Reduction</u>\n",
    "    Dimensionality reduction reduces higher dimension to lower dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithms will be discussed <br>\n",
    "\n",
    "##### Supervised learning  \n",
    "<ol>\n",
    "    <li>Linear Regression</li>\n",
    "    <li>Ridge and Lasso</li>\n",
    "    <li>Logistic Regression</li>\n",
    "    <li>Decision Tree</li>\n",
    "    <li>AdaBoost</li>\n",
    "    <li>Random Forest</li>\n",
    "    <li>Gradient Boosting</li>\n",
    "    <li>Xgboost</li>\n",
    "    <li>Naive Bayes</li>\n",
    "    <li>Support Vector Machine, SVM</li>\n",
    "    <li>KNN</li>\n",
    "</ol>\n",
    "<br>\n",
    "\n",
    "##### Unsupervised learning  \n",
    "<ol>\n",
    "    <li>K means</li>\n",
    "    <li>DBScan</li>\n",
    "    <li>Hierarchical Clustering</li>\n",
    "    <li>K Nearest Clustering</li>\n",
    "    <li>Principal Component Analysis, PCA</li>\n",
    "    <li>Latent Dirichlet Allocation, LDA</li>\n",
    "    <li>Gradient Boosting</li>\n",
    "    <li>Xgboost</li>\n",
    "    <li>Naive Bayes</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Linear Regression\n",
    "<br>\n",
    "  \n",
    "  Train dataset --> Model --> Hypothesis (new input --> predict output)\n",
    "  \n",
    "  Model : $ h_{\\theta}(x) = \\theta_0 + \\theta_1x$ \n",
    " \n",
    " where $\\theta_0$ and  $\\theta_1$ are weights\n",
    "\n",
    "  Purpose is the minimize the distance between the data points and the best fit line.\n",
    "  \n",
    "  <u>Cost function</u>\n",
    "  \n",
    "  $J(\\theta_0,\\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x_i) - y_i)^2$\n",
    "  \n",
    "  This cost function is called Squared Error Function\n",
    "  \n",
    "$$minimize_{\\theta_0,\\theta_1}    \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x_i) - y_i)^2$$\n",
    " \n",
    "$$minimize_{\\theta_0,\\theta_1}    J(\\theta_0,\\theta_1)$$\n",
    "\n",
    "![cost function graph](cost_curve.png \"cost function graph\")\n",
    "\n",
    " <u>Convergence Algorithms</u>\n",
    " \n",
    " repeat until convergence\n",
    " \n",
    "   {\n",
    " \n",
    " $$\\theta_j := \\theta_j - \\alpha \\frac{\\delta}{\\delta\\theta_j}J(\\theta_0,\\theta_1)$$\n",
    " \n",
    "   }\n",
    "   \n",
    "$\\alpha$ is the learning rate, usually 0.01.\n",
    "\n",
    "<u>Gradient Descrent Algorithm</u>\n",
    "\n",
    " repeat until convergence\n",
    " \n",
    "   {\n",
    " \n",
    " $$\\theta_j := \\theta_j - \\alpha \\frac{\\delta}{\\delta\\theta_j}J(\\theta_0,\\theta_1)$$\n",
    " \n",
    "   }\n",
    "<br><br>\n",
    "Performing derivatives\n",
    "\n",
    "   $$\\frac{\\delta}{\\delta\\theta_j}J(\\theta_0,\\theta_1) = \\frac{\\delta}{\\delta\\theta_j} \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x_i) - y_i)^2$$\n",
    "<br><br>\n",
    "   \n",
    "$ j = 0$ <br>\n",
    "   $$\\frac{\\delta}{\\delta\\theta_0}J(\\theta_0,\\theta_1) = \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x_i) - y_i)$$\n",
    "   \n",
    "<br><br>\n",
    "$ j = 1 $<br>\n",
    "   $$\\frac{\\delta}{\\delta\\theta_1}J(\\theta_0,\\theta_1) = \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x_i) - y_ix_i$$\n",
    "<br><br>   \n",
    "repeat until convergence\n",
    " \n",
    "   {\n",
    " \n",
    " $$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)})$$ <br>\n",
    " $$\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)}) - y^{(i)})x^{(i)}$$\n",
    " \n",
    "   }\n",
    "   \n",
    "<br><br>\n",
    "\n",
    "<u>Performance Metrics</u>\n",
    "\n",
    "$R^2$ and Adjusted $R^2$\n",
    "\n",
    "$$R^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum(y_i - \\hat{y})^2}{\\sum(y_i - \\bar{y})^2}$$\n",
    "<br>\n",
    "Where RSS is sum of squares of residuals, TSS is total sum of squares, $\\hat{y}$ is the y of the best fit line and $\\bar{y}$ is the mean of y values. <br><br>\n",
    "\n",
    "![R-square graph](R_square_curve.jpg 'R-square graph')\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<u>Adjusted R-square</u><br><br>\n",
    "Adjusted R-square is used because some of non-correlated features improve the R-square where they don't affect the outcomes. <br><br>\n",
    "\n",
    "$$Adjusted\\ R^2 = 1 - \\frac{(1-R^2)(N-1)}{N - p -1}$$ <br><br>\n",
    "\n",
    "Where N is the sample size and p is number of features/predictors/independent variables. <br><br>\n",
    "\n",
    "Between $R^2$ and $Adjusted\\ R^2$, $R^2$ is always bigger than $Adjusted\\ R^2$ because as more features added $(N - p - 1)$ will get smaller. With $R^2$, if the features are correlated, it would have a huge increase. However, when the features are not correlated, it would have a little increase. <br><br>\n",
    "\n",
    "<u>Overfitting</u>\n",
    "\n",
    "Model performs well with training data indicating low bias but it fails to perform well with test data indicating high variance. <br>\n",
    "\n",
    "<u>Underfitting</u>\n",
    "\n",
    "Model performs poorly with both traning data and test data, indicating high bias and high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ridge and Lasso Regression\n",
    "\n",
    "Ridge and Lasso regression are some of the simple techniques to reduce model complexity and prevent overfitting which may result from simple linear regression. They are added to the cost function to penalize the model for large weights and biases. Larger weights and biases tend to overfitting.<br><br>\n",
    "\n",
    "Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting. <br><br>\n",
    "\n",
    "<u>Ridge Regression (L2 Regularization)</u>\n",
    "\n",
    "Ridge regression introduces a small amount bias to our model such that the new model doesn't fit the training data as well. \n",
    "\n",
    "![linear regression](Ridge1.png 'linear regression')\n",
    "![Ridge regression](Ridge2.png 'ridge regression')\n",
    "\n",
    "The red line fits the training data perfectly but it produces a high variance for the test data, green dots. The blue line, Ridge regression, fits the test data data with a compromise of the training data; hence, it produces a smaller variance. Thus, Ridge regression aims to improve generalization. <br><br>\n",
    "\n",
    "Model for one feature and m observations: $$h_{\\theta}(x) = \\theta_0 + \\theta_1x $$\n",
    "\n",
    "Ridge cost function: <br><br>\n",
    "$$J(\\theta_0,\\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}(y_i - h_{\\theta}(x_i))^2 + \\lambda\\theta_1^2$$\n",
    "\n",
    "In general, if there are $p$ features and $m$ features, then <br><br>\n",
    "Model: $$h_{\\theta}(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_px_p $$\n",
    "$$h_{\\theta}(x) = \\theta_0 + \\sum_{j=1}^{p}\\theta_jx_j$$\n",
    "\n",
    "Ridge cost function:\n",
    "$$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(y_i - \\theta_0 - \\sum_{j=1}^{p}\\theta_{ij}x_{ij})^2 + \\frac{\\lambda}{2m}\\sum_{j=1}^{p}\\theta_j^2$$\n",
    "\n",
    "Where $\\lambda$ is hyperparameter.\n",
    "<br><br>\n",
    "\n",
    "To get $\\theta$'s, the cost function needs minimized.\n",
    "\n",
    "$$\n",
    "argmin_{\\theta \\in \\mathbb{R}}\\ J(\\theta)\n",
    "$$\n",
    "$$\n",
    "argmin_{\\theta \\in \\mathbb{R}}\\ \\frac{1}{2m}\\sum_{i=1}^{m}(y_i - \\theta_0 - \\sum_{j=1}^{p}\\theta_{ij}x_{ij})^2 + \\frac{\\lambda}{2m}\\sum_{j=1}^{p}\\theta_j^2\n",
    "$$\n",
    "<br> $$or$$\n",
    "\n",
    "$$\n",
    "argmin_{\\theta \\in \\mathbb{R}}\\ \\parallel \\mathbf{y} - \\mathbf{X\\theta}\\ \\parallel _2 ^2 + \\mathbf {\\lambda \\parallel\\theta\\ \\parallel} _2 ^2\n",
    "$$\n",
    "\n",
    "Where $\\parallel\\theta\\ \\parallel _2$ is L2 norm of $\\theta$ defined as,\n",
    "\n",
    "$$\\parallel\\theta\\ \\parallel _2 = \\sqrt{\\theta_1^2 + \\theta_2^2 + \\theta_3^2 + ... +\\theta_p^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
