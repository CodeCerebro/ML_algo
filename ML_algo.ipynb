{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wch.github.io/latexsheet/latexsheet.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "    <li>Introduction to machine learning</li>\n",
    "    <li> Supervised ML and Unsupervised ML</li>\n",
    "    <li>Linear Regression</li>\n",
    "    <li>$R^2$ and Abjusted $R^2$</li>\n",
    "    <li>Ridge and Lasso Regression</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Machine Learning <br>\n",
    "\n",
    "\n",
    "#### <u>AI application</u>\n",
    "    AI application is able to do its own tasks without any human intervention.\n",
    "    eg. Netflix, Amazon, self-driving cars\n",
    "    \n",
    "#### <u>Machine learning</u>\n",
    "    Machine learning is a subset of AI that used statistics to analyze, visualize, prediction and forecasting.\n",
    "   \n",
    "#### <u>Deep learning</u>\n",
    "    Deep learning is a subset to machine learning that mimics human's brain through multilayers neural network.\n",
    "    \n",
    "In machine learning and deep learning, there are supervised learning, unsupervised learning and reinforcement. In supervised learning, there are regression, and classification techniques. In unsupervised learning, there are clustering, and dimensionality reduction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Learning\n",
    "\n",
    "Models are built based on the independent features (inputs) and dependent features (outputs)\n",
    "\n",
    "##### <u>Regression</u> \n",
    "    If the dependent features (outputs) is continuous, then the problem becomes a regression problem\n",
    "    \n",
    "##### <u>Classification</u>\n",
    "    If the dependent features (outputs) is a fixed number of categories, then the problem becomes a classification problem.\n",
    "    eg. binary classification (2 categories)\n",
    "        multiclass classification (more than 2 categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Unsupervised Learning\n",
    "\n",
    "Models are built based on the independent features. Here there are no dependent features\n",
    "\n",
    "#####  <u>Clustering</u>\n",
    "    Clustering groups observations with similar features into groups\n",
    "    e.g. customer segmentation\n",
    "#####  <u>Dimensionality Reduction</u>\n",
    "    Dimensionality reduction reduces higher dimension to lower dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithms will be discussed <br>\n",
    "\n",
    "##### Supervised learning  \n",
    "<ol>\n",
    "    <li>Linear Regression</li>\n",
    "    <li>Ridge and Lasso</li>\n",
    "    <li>Logistic Regression</li>\n",
    "    <li>Decision Tree</li>\n",
    "    <li>AdaBoost</li>\n",
    "    <li>Random Forest</li>\n",
    "    <li>Gradient Boosting</li>\n",
    "    <li>Xgboost</li>\n",
    "    <li>Naive Bayes</li>\n",
    "    <li>Support Vector Machine, SVM</li>\n",
    "    <li>KNN</li>\n",
    "</ol>\n",
    "<br>\n",
    "\n",
    "##### Unsupervised learning  \n",
    "<ol>\n",
    "    <li>K means</li>\n",
    "    <li>DBScan</li>\n",
    "    <li>Hierarchical Clustering</li>\n",
    "    <li>K Nearest Clustering</li>\n",
    "    <li>Principal Component Analysis, PCA</li>\n",
    "    <li>Latent Dirichlet Allocation, LDA</li>\n",
    "    <li>Gradient Boosting</li>\n",
    "    <li>Xgboost</li>\n",
    "    <li>Naive Bayes</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Linear Regression\n",
    "<br>\n",
    "  \n",
    "  Train dataset --> Model --> Hypothesis (new input --> predict output)\n",
    "  \n",
    "  Model : $ h_{\\theta}(x) = \\theta_0 + \\theta_1x$ \n",
    " \n",
    " where $\\theta_0$ and  $\\theta_1$ are weights\n",
    "\n",
    "  Purpose is the minimize the distance between the data points and the best fit line.\n",
    "  \n",
    "  <u>Cost function</u>\n",
    "  \n",
    "  $J(\\theta_0,\\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})^2$\n",
    "  \n",
    "  This cost function is called Squared Error Function\n",
    "  \n",
    "$$minimize_{\\theta_0,\\theta_1}    \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})^2$$\n",
    " \n",
    "$$minimize_{\\theta_0,\\theta_1}    J(\\theta_0,\\theta_1)$$\n",
    "\n",
    "![cost function graph](cost_curve.png \"cost function graph\")\n",
    "\n",
    " <u>Convergence Algorithms</u>\n",
    " \n",
    " repeat until convergence\n",
    " \n",
    "   {\n",
    " \n",
    " $$\\theta_j := \\theta_j - \\alpha \\frac{\\delta}{\\delta\\theta_j}J(\\theta_0,\\theta_1)$$\n",
    " \n",
    "   }\n",
    "   \n",
    "$\\alpha$ is the learning rate, usually 0.01.\n",
    "\n",
    "<u>Gradient Descrent Algorithm</u>\n",
    "\n",
    " repeat until convergence\n",
    " \n",
    "   {\n",
    " \n",
    " $$\\theta_j := \\theta_j - \\alpha \\frac{\\delta}{\\delta\\theta_j}J(\\theta_0,\\theta_1)$$\n",
    " \n",
    "   }\n",
    "<br><br>\n",
    "Performing derivatives\n",
    "\n",
    "   $$\\frac{\\delta}{\\delta\\theta_j}J(\\theta_0,\\theta_1) = \\frac{\\delta}{\\delta\\theta_j} \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})^2$$\n",
    "<br><br>\n",
    "   \n",
    "$ j = 0$ <br>\n",
    "   $$\\frac{\\delta}{\\delta\\theta_0}J(\\theta_0,\\theta_1) = \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})$$\n",
    "   \n",
    "<br><br>\n",
    "$ j = 1 $<br>\n",
    "   $$\\frac{\\delta}{\\delta\\theta_1}J(\\theta_0,\\theta_1) = \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})x^{(i)}$$\n",
    "<br><br>   \n",
    "repeat until convergence\n",
    " \n",
    "   {\n",
    " \n",
    " $$\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})$$ <br>\n",
    " $$\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)} - y^{(i)})x^{(i)}$$\n",
    " \n",
    "   }\n",
    "   \n",
    "<br><br>\n",
    "\n",
    "<u>Performance Metrics</u>\n",
    "\n",
    "$R^2$ and Adjusted $R^2$\n",
    "\n",
    "$$R^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum(y_i - \\hat{y})^2}{\\sum(y_i - \\bar{y})^2}$$\n",
    "<br>\n",
    "Where RSS is sum of squares of residuals, TSS is total sum of squares, $\\hat{y}$ is the y of the best fit line and $\\bar{y}$ is the mean of y values. <br><br>\n",
    "\n",
    "![R-square graph](R_square_curve.jpg 'R-square graph')\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<u>Adjusted R-square</u><br><br>\n",
    "Adjusted R-square is used because some of non-correlated features improve the R-square where they don't affect the outcomes. <br><br>\n",
    "\n",
    "$$Adjusted\\ R^2 = 1 - \\frac{(1-R^2)(N-1)}{N - p -1}$$ <br><br>\n",
    "\n",
    "Where N is the sample size and p is number of features/predictors/independent variables. <br><br>\n",
    "\n",
    "Between $R^2$ and $Adjusted\\ R^2$, $R^2$ is always bigger than $Adjusted\\ R^2$ because as more features added $(N - p - 1)$ will get smaller. With $R^2$, if the features are correlated, it would have a huge increase. However, when the features are not correlated, it would have a little increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
